
\title{Suggestions for Improved Stimulus Reporting and Manipulation Checks in Video Game Research}

 “Writing about music is like dancing about architecture.” – Marty Mull (uncertain: http://quoteinvestigator.com/2010/11/08/writing-about-music/)
Current Problems 

Research in violent video game effects on aggressive outcomes is a controversial field. Part of this controversy stems from differing results across research teams. Some teams are able to (conceptually) replicate the effect, while other teams do not observe the effect. These differences have become somewhat personal recently, as \citet{Greitemeyer:Mugge:2014} present meta-analytic results demonstrating differing effect size estimates across research teams, suggesting that certain researchers aren't ``doing it right''. %I don't want to get into a mudfight here so this will need editing.

This controversy could be alleviated if research were more replicable across research teams. Replicability of research has been the focus of much recent attention and work. These reproducibility initiatives have emphasized the importance of preregistration of studies, open availability of data, and the open availability of research materials. %Research that meets these criteria is more replicable, and therefore, more convincing, and more likely to reflect true phenomena.
It is this last point, the availability of research materials, that we focus on. In conventional psychological research, open materials means making available the questionnaires, measures, manipulations, and computer scripts necessary to perform a close replication of the experiment. It is well-understood that the methods and materials of experiments are rarely described with sufficient precision to permit such a close replication. %Reproducibility is 

Because of the complex, multifaceted, and interactive nature of video games, it requires careful consideration to determine what game content has been expressed in an experimental manipulation. A close understanding of the content is necessary for researchers, their reviewers, and their readers in determining the validity and efficacy of the manipulation, as content which is unnoticed by the experimenters may nevertheless influence participants and the results of the study.

The standards for reporting game content are substandard. In most fields of research, scientists carefully report all their stimuli and all relevant dimensions of these stimuli. For example, in studies involving emotional images, reviewers and readers would be presented with the set of stimuli, whether as attached images or as reference numbers from a set such as the IAPS. This practice allows reviewers and readers to assess and evaluate whether the stimulus is appropriate to the experimental condition. 

However, in media research, the used stimuli are often much less transparent. Researchers often provide little more than a name, which is especially unfortunate in the case that the game name is indistinct and difficult to find via web search (e.g. “Penguins” (Greitemeyer, 2014) or “Austin Powers” (Bushman & Anderson, 2002)). This obfuscates the nature of the game and impairs communication to the reader.

There are also no standards for reporting the portion of game content experienced. A single video game often contains many hours’ worth of content, with many different levels, missions, game modes, and optional settings. All of these conditions may be potentially important to the manipulation, and failures to reproduce any of these conditions may lead to failures of replication. 

Many popular video games also begin with an hour or more of tutorials, cutscenes, and dialogue. In the case that the player is put in front of the game and the game started from the very beginning, it is actually quite unlikely that the player will even finish watching the opening movie sequences within fifteen minutes! 

Because games are interactive, players may also have the opportunity to choose which content they experience and how their avatar behaves. For example, popular “open-world” games such as Grand Theft Auto, Red Dead Redemption, or The Elder Scrolls V: Skyrim allow players an open world composed of many possible activities. While violence is perhaps the most common of these activities, it is not impossible that some players may spend the entire play session engaged in a lawful and non-violent pastime such as bowling or horseback riding.

The above ambiguities make it difficult to determine the exact game content that participants experience in most studies. This uncertainty makes it difficult to evaluate and replicate other researchers' work. It also obscures the presence of game contents besides violent content, glossing over potential confounds and moderators.

It is also sometimes unclear as to whether experimental manipulations are successful across all participants. First, not all subjects may experience the game as intended. Some participants may quickly understand the controls, while other participants may fail to understand the basic rules of play or otherwise fail to engage with the experimental manipulation. Some participants may disregard the instructions of the experimenter or even the game itself in order to interact with the software in their own way. Second, aggression research frequently involves deception, which does not always work on participants. It is often unclear as to whether the deception was effective on all subjects, or if some subjects were wise to the deception. In the latter case, it is often not clear what was done with these subjects. While some research projects report testing for suspicion and an exclusion of subjects who were not deceived, many studies still do not report any such test or what would be done with the data of participants failing such a test. It cannot be assumed that non-deceived participants exhibit representative levels of aggression; even if they did, it would seem to bode ill for the validity of the dependent variable.

\section{Report Everything}
Provide your stimuli. As a service to collaborators, reviewers, and readers, we suggest that researchers either publish or link relevant video footage of all games used in game manipulations. Vast internet communities of hobbyist game players have already published gameplay for thousands of games to websites such as YouTube.com, Vimeo.com, and Twitch.TV. If relevant footage is not already available, researchers can record their own using screencapture video software such as OpenBroadcasterSoftware (obsproject.com) or FRAPS (www.fraps.com). Including links to such footage makes it easier for your reviewers and readers to assess the content (both graphical and ludological) of your game manipulation. Authors are encouraged to footnote any mentioned game with a video link, when appropriate. Authors are also reminded that there exists an APA format for citing software and that video games are software. Supply the full name including subtitle and year of publication. Provide a URL, when applicable.

\subsection{Observe and report player behavior} In experimental psychology, it is common to perform a manipulation check to assess the degree to which an experimental manipulation has had the intended effect. When studying games, it is similarly important to observe player behavior and performance, as this can indicate whether your participants are appropriately perceiving and interacting with the game as intended. Gameplay variables thereby often represent potentially-important mediator variables.

Gameplay variables may also represent individual differences among your players. If one player as compared to the rest of the sample clears more stages, suffers less damage per stage, and earns more points per stage, it can be inferred that player is more skilled than most. Conversely, if another player demonstrates extremely poor progress towards the game’s goal, it can be inferred that the player has failed (to some degree) to understand the controls and strategy, invest sufficient effort, and/or experience the intended game content. 

The collection and analysis of these variables is often quite easy thanks to the computerized nature of these games. For example, many games automatically keep track of score, accuracy of attacks, and other gameplay variables. In the event that these variables are not measured and provided, it is often possible to modify the game’s scripts to track these variables. One open-source game maniuplation (Hilgard, 2014) tracks the number of times the player died, the number of monsters slain, the player’s furthest progress through the game, the number of bullets fired with each weapon, and the total number of times the player has been hit. These measures help to identify experimental failures, such as if the subject decided to run past all the monsters instead of fight them, ignored the game, or failed to learn the controls or rules of the game.

Observing the player’s in-game behaviors may also yield new insights to the mechanisms of the effects of game manipulations. Through observing the interaction between player and the player-dependent game content, researchers can peer inside the black box of game manipulations. This may help to answer persistent questions of who is affected, how, and why.

We urge researchers to use these gameplay variables in analysis to test for alternative hypotheses, failures of random assignment, and for other effects of subject. However, we caution against the use of in-game behavior as a measure of aggression in itself, as these behaviors are more likely to be motivated by strategic concerns, and thus unlikely to have external validity in predicting real-world aggression. We would not say that a Chess player who preserves his bishops but sacrifices his queen has positive attitudes towards religion and negative attitudes towards women! Thus, a tendency for players to use a sword instead of martial arts to kill an opponent (Barlett et al., 2008) or to use weapons instead of stealth (Anderson & Morrow, 1995; Panee & Ballard, 2002) are unlikely to represent changes in real-world aggression. (Note, however, that Barlett et al. still found significant effects on other, more valid, measures of aggressive arousal and cognition.)
	
Always test subjects for suspicion and report any exclusions. It cannot be assumed that all subjects were naïve to the study hypotheses, nor can it be assumed that the inclusion of non-naïve participants does not alter the observed effect. Researchers must test participants for failures of deception and report any exclusions made for this reason . Ideally, the field would decide upon a standardized procedure for screening and including or excluding participants.

\section{Applications}
These improved reporting standards will yield greater transparency in communicating research materials between researcher and reader. Thus, the entire breadth of differences between game conditions will be clearly communicated. Furthermore, observing player behavior during gameplay will provide additional insights into the behaviors, cognitions, and emotions brought about by game manipulations, as well as indicate when the manipulations fail. This will improve the efficacy of manipulations, the precision and reliability of effect size estimates, the replication of manipulations across laboratories, and the opportunities to observe mediating variables .
